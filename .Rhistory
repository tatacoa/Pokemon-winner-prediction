biplot(pr.clustermat)
plot3d(pr.clustermat)
install.packages("plot3D")
plot3d(pr.clustermat)
set.seed=(10)
x=matrix(rnorm(75*3),ncol=3); x
x[1:25,1]=x[1:25,1]+5; x
x[51:75,2]=x[51:75,2]-6; x
truth<-rep(1:3,c(25,25,25)); truth
pairs(x,col=truth+1)
# Run K-means with 3 clusters
kmeans3 <- kmeans(x, centers = 3, nstart=20); kmeans3
#Compare the output clusters with the known clusters
table(kmeans3$cluster, truth)
#Colour the clusters and add the cluster means
plot3d(x,col=kmeans3$cluster+1)
plot3d(kmeans3$centers,add=TRUE,col=2:4,type="s")
#Repeat the above using just two clusters.
#How can you describe the two clusters in one sentence?
# the clusters seems good to me!
kmeans2 <- kmeans(x, centers = 2, nstart=20); kmeans2
plot3d(x,col=kmeans2$cluster+1)
plot3d(kmeans2$centers,add=TRUE,col=2:4,type="s")
#Generate a new matrix called y with 10 columns instead of 3
y=matrix(rnorm(75*10),ncol=10); dim(y)
y[1:25,1]=y[1:25,1]+5; y
y[51:75,2]=y[51:75,2]-6; y
#define the clusters in exactly the same way as above
truthy<-rep(1:3,c(25,25,25)); truthy
pairs(y,col=truth+1)
plot3d(y)
#Run kmeans with 3 centres on x and on y
kmeansy3 <- kmeans(y, centers = 3, nstart=20); kmeansy3
plot3d(y,col=kmeansy3$cluster+1)
plot3d(kmeansy3$centers,add=TRUE,col=2:4,type="s")
#Use the R function table to compare how many rows have been
#correctly and incorrectly assigned
table(kmeansy3$cluster, truthy)
help(USArrests)
summary(USArrests)
head(USArrests)
# Obtain names of the four variables in USArrests
colnames(USArrests)
# For each variable obtain the mean and the standard deviation
summary(USArrests)
# Use prcomp to get the principal components for this dataset
?prcomp
pr.out <- prcomp(USArrests, scale = TRUE)
# What is the difference between pr.out$scale and pr.out$sd?
pr.out$scale; # centrando las variables???
pr.out$sd # Standart deviations
summary(pr.out)
plot(pr.out$sdev^2, type = "b")
abline(h=1)
#biplot
biplot(pr.out,xlabs=state.abb)
# Run K-means on the PCA data with 2 clusters and plot the results
km.out<-kmeans(pr.out$x,centers=2,nstart=20)
plot(pr.out$x[,1:2],type="n")
text(pr.out$x[,1],pr.out$x[,2],labels=state.abb,col=km.out$cluster)
# How many states are in each cluster: 20 and 30
table(km.out$cluster)
wss.vec<-rep(NA,10)
for(k in 1:10){
km.outs<-kmeans(pr.out$x[,1:2],centers=k,nstart=20)
wss.vec[k]<-km.outs$tot.withinss
}
plot(wss.vec,type="b")
km.out4 <-kmeans(pr.out$x[,1:2],centers=4,nstart=20)
plot(pr.out$x[,1:2],type="n")
text(pr.out$x[,1],pr.out$x[,2],labels=state.abb,col=km.out4$cluster)
# Exercise 3: Hamburg decathlon data, PCA and clustering
decathlon <- read.csv2("/Users/tata/Downloads/Zehnkampf2017Hamburg.csv",
stringsAsFactors = FALSE, fileEncoding = "ISO-8859-1", header = TRUE, sep = ";")[-c(1:3,seq(10,28,2))]
## transform times to seconds
hilf <- decathlon$Zeit.400m
decathlon$Zeit.400m <- 60*as.numeric(substr(hilf,1,2)) +
as.numeric(gsub(",",".",substr(hilf,4,nchar(hilf))))
hilf <- decathlon$Zeit.1500m
decathlon$Zeit.1500m <- 60*as.numeric(substr(hilf,1,2)) +
as.numeric(gsub(",",".",substr(hilf,4,nchar(hilf))))
colnames(decathlon) <- c("YearOfBirth", "Class", "Points",
"Day1", "Day2", "Time.100m", "LongJump", "ShotPut",
"HighJump", "Time.400m", "Hurdles", "Discus", "PoleVault",
"JavelinThrow", "Time.1500m")
#remove any competitors without complete data, and select just the event results
temp <- apply(decathlon,1,function(x) sum(is.na(x)))
clustermat<-decathlon[temp==0,6:15]
pairs(clustermat)
pr.clustermat <- prcomp(clustermat, scale = TRUE); pr.clustermat
summary(pr.clustermat)
plot(pr.clustermat$sdev^2, type = "b")
abline(h=1)
#biplot
biplot(pr.clustermat)
plot3d(pr.clustermat)
# Create a cumulative variance plot for the principal components.
# 3 PCs is probably a good choice
summary(pr.clustermat)
plot(pr.clustermat$sdev^2, type = "b")
abline(h=1)
# carry out a k-means cluster analysis on these data
km.clustermat<-kmeans(pr.clustermat$x,centers=2,nstart=20)
plot(pr.clustermat$x[,1:2],type="n")
text(pr.clustermat$x[,1],pr.clustermat$x[,2],col=km.clustermat$cluster)
# Create a cumulative variance plot for the principal components.
# 3 PCs is probably a good choice
summary(pr.clustermat)
plot(pr.clustermat$sdev^2, type = "b")
abline(h=1)
# carry out a k-means cluster analysis on these data
km.clustermat<-kmeans(pr.clustermat$x,centers=3,nstart=20)
plot(pr.clustermat$x[,1:2],type="n")
text(pr.clustermat$x[,1],pr.clustermat$x[,2],col=km.clustermat$cluster)
find.package("cluster")
library(cluster)
find.package("ISLR")
install.packages("ISLR")
library(ISLR)
help(USArrests)
summary(USArrests)
head(USArrests)
colnames(USArrests)
row.names(USArrests)<-state.abb
pairs(USArrests)
pam(USArrests,k=4)
clusplot(pam.out,labels=3)
pam.out<-pam(USArrests,k=4)
clusplot(pam.out,labels=3)
?silhouette
sp<-silhouette(pam.out)
plot(sp,col=1:4)
mean(sp[,"sil_width"])
abline(v=mean(sp[,"sil_width"]))
sp
# The mean silhouette width can be used to assess which is the best number of clusters
avesw.vec<-rep(NA,7)
for(k in 2:7)
avesw.vec[k]<-mean(silhouette(pam(USArrests,k=k))[,"sil_width"])
plot(1:7,avesw.vec,type="b",ylim=c(0,0.6))
# The mean silhouette width can be used to assess which is the best number of clusters
avesw.vec<-rep(NA,7)
for(i in 2:7)
avesw.vec[i]<-mean(silhouette(pam(USArrests,k=i))[,"sil_width"])
plot(1:7,avesw.vec,type="b",ylim=c(0,0.6))
avesw.vec
# Rerun the PAM algorithm with the optimal number of clusters
pam.out<-pam(USArrests,k=2)
clusplot(pam.out,labels=3)
sp<-silhouette(pam.out)
plot(sp,col=1:2)
abline(v=mean(sp[,"sil_width"]))
# Compare the optimal PAM clustering with the K-Means result
km.out<-kmeans(USArrests,centers=2,nstart=20)
# Compare the optimal PAM clustering with the K-Means result
km.out<-kmeans(USArrests,centers=2,nstart=20); km.out
pam.out
pam.out$x
pam.out$clustering
# Compare the optimal PAM clustering with the K-Means result
km.out<-kmeans(USArrests,centers=2,nstart=20); km.out
km.out$x
km.out$cluster
table(km.out$cluster,pam.out$clustering)
km.out$cluster
pam.out$clustering
km.out$cluster == pam.out$clustering
# silhouette using kmeans as input
sp<-silhouette(km.out$cluster, dist(USArrests))
plot(sp,col=1:2)
decathlon <- read.csv2("/Users/tata/Downloads/Zehnkampf2017Hamburg.csv",
stringsAsFactors = FALSE, fileEncoding = "ISO-8859-1", header = TRUE, sep = ";")[-c(1:3,seq(10,28,2))]
# transform times to seconds
hilf <- decathlon$Zeit.400m
decathlon$Zeit.400m <- 60*as.numeric(substr(hilf,1,2)) +
as.numeric(gsub(",",".",substr(hilf,4,nchar(hilf))))
hilf <- decathlon$Zeit.1500m
decathlon$Zeit.1500m <- 60*as.numeric(substr(hilf,1,2)) +
as.numeric(gsub(",",".",substr(hilf,4,nchar(hilf))))
colnames(decathlon) <- c("YearOfBirth", "Class", "Points",
"Day1", "Day2", "Time.100m", "LongJump", "ShotPut",
"HighJump", "Time.400m", "Hurdles", "Discus", "PoleVault",
"JavelinThrow", "Time.1500m")
#remove any competitors without complete data, and select just the event results
temp <- apply(decathlon,1,function(x) sum(is.na(x)))
clustermat<-decathlon[temp==0,6:15]
pairs(clustermat)
pam(scale(clustermat) , k=4)
pam.out<-pam(scale(clustermat) , k=4)
clusplot(pam.out,labels=0)
pam.out<-pam(scale(clustermat) , k=4)
clusplot(pam.out,labels=0)
pam.out<-pam(scale(clustermat) , k=4)
clusplot(pam.out,labels=0)
pam.out<-pam(scale(clustermat) , k=3)
clusplot(pam.out,labels=0)
pam.out<-pam(scale(clustermat) , k=3)
clusplot(pam.out,labels=0)
# PAM applyied to Decathlon data
clustermat_scaled <- scale(clustermat)
clustermat_scaled
pam.out<-pam(clustermat_scaled , k=3)
clusplot(pam.out,labels=0)
# Rerun the PAM algorithm with the optimal number of clusters
sp<-silhouette(pam.out)
plot(sp,col=1:2)
abline(v=mean(sp[,"sil_width"]))
# The mean silhouette width can be used to assess which is the best number of clusters
avesw.vec<-rep(NA,7)
for(i in 2:7)
avesw.vec[i]<-mean(silhouette(pam(clustermat_scaled,k=i))[,"sil_width"])
plot(1:7,avesw.vec,type="b",ylim=c(0,0.6))
avesw.vec # cual tiene el largest mean silhouette width? 2???
# PAM applyied to Decathlon data
clustermat_scaled <- scale(clustermat); clustermat_scaled
pam.out<-pam(clustermat , k=3)
clusplot(pam.out,labels=0)
# Rerun the PAM algorithm with the optimal number of clusters
sp<-silhouette(pam.out)
plot(sp,col=1:2)
abline(v=mean(sp[,"sil_width"])) # hay varios valores negativos en el Silhouet
# The mean silhouette width can be used to assess which is the best number of clusters
avesw.vec<-rep(NA,7)
for(i in 2:7)
avesw.vec[i]<-mean(silhouette(pam(clustermat,k=i))[,"sil_width"])
plot(1:7,avesw.vec,type="b",ylim=c(0,0.6))
avesw.vec # cual tiene el largest mean silhouette width? todas son muy malas menos de 0.4
help(USArrests)
summary(USArrests)
head(USArrests)
colnames(USArrests)
row.names(USArrests)<-state.abb
pairs(USArrests)
# PAM clustering with 4 clusters
pam.out<-pam(USArrests,k=4)
clusplot(pam.out,labels=3)
# silhouette width over 0.4 is good
?silhouette
sp<-silhouette(pam.out); sp
plot(sp,col=1:4)
mean(sp[,"sil_width"])
abline(v=mean(sp[,"sil_width"]))
# The mean silhouette width can be used to assess which is the best number of clusters
avesw.vec<-rep(NA,7)
for(i in 2:7)
avesw.vec[i]<-mean(silhouette(pam(USArrests,k=i))[,"sil_width"])
plot(1:7,avesw.vec,type="b",ylim=c(0,0.6))
avesw.vec # cual tiene el largest mean silhouette width? 2???
# Rerun the PAM algorithm with the optimal number of clusters
pam.out<-pam(USArrests,k=2); pam.out
clusplot(pam.out,labels=3)
sp<-silhouette(pam.out)
plot(sp,col=1:2)
abline(v=mean(sp[,"sil_width"]))
Element <- c(1:6)
X1 <- c(4, 5, 6, 10, 14, 15)
X2 <- c(2, ,5, 11, 2, 7, 9)
dat <- cbind(Element, X1, X2)
Element <- c(1:6)
X1 <- c(4, 5, 6, 10, 14, 15)
X2 <- c(2, ,5, 11, 2, 7, 9)
dat <- cbind(Element, X1, X2)
Element <- c(1:6)
X1 <- c(4, 5, 6, 10, 14, 15)
X2 <- c(2, ,5, 11, 2, 7, 9)
dat <- cbind(Element, X1, X2)
Element
X1
X2
X2 <- c(2, ,5, 11, 2, 7, 9)
X3 <- c(2, ,5, 11, 2, 7, 9)
X1 <- c(4, 5, 6, 10, 14, 15)
X2 <- c(2, 5, 11, 2, 7, 9)
dat <- cbind(Element, X1, X2)
dat <- cbind(Element, X1, X2); dat
dist(dat)
?dist
dist(dat, digits = 1)
dist(dat[,2:3])
dist(dat)
dat
dist(round(dat, digits = 1)
)
round(dist(dat, digits = 1))
round(dist(dat), digits = 1)
nci.labs=NCI60$labs
nci.data=NCI60$data
dim(nci.data)
table(nci.labs)
Cols=function(vec){
cols=rainbow(length(unique(vec)))
return(cols[as.numeric(as.factor(vec))]) }
# plot the principal component score vectors
par(mfrow=c(1,2))
plot(pr.out$x[,1:2], col=Cols(nci.labs), pch=19,
xlab="Z1",ylab="Z2")
plot(pr.out$x[,c(1,3)], col=Cols(nci.labs), pch=19,
xlab="Z1",ylab="Z3")
pr.out=prcomp(nci.data, scale=TRUE)
Cols=function(vec){
cols=rainbow(length(unique(vec)))
return(cols[as.numeric(as.factor(vec))]) }
# plot the principal component score vectors
par(mfrow=c(1,2))
plot(pr.out$x[,1:2], col=Cols(nci.labs), pch=19,
xlab="Z1",ylab="Z2")
plot(pr.out$x[,c(1,3)], col=Cols(nci.labs), pch=19,
xlab="Z1",ylab="Z3")
summary(pr.out)
plot(pr.out)
pve=100*pr.out$sdev^2/sum(pr.out$sdev^2)
par(mfrow=c(1,2))
plot(pve, type="o", ylab="PVE", xlab="Principal Component",
col =" blue ")
plot(cumsum(pve), type="o", ylab="Cumulative PVE", xlab="
Principal Component ", col =" brown3 ")
# PVE can be also computed like this<
summary(pr.out)$importance[2,]
summary(pr.out)$importance[3,]
plot(cumsum(pve), type="o", ylab="Cumulative PVE", xlab="
Principal Component ", col =" brown3 ")
plot(pve, type="o", ylab="PVE", xlab="Principal Component",
col =" blue ")
plot(cumsum(pve), type="o", ylab="Cumulative PVE", xlab="
Principal Component ", col =" brown3 ")
# PVE can also be computed like this:
summary(pr.out)$importance[2,]
par(mfrow=c(1,2))
plot(pve, type="o", ylab="PVE", xlab="Principal Component",
col =" blue ")
plot(cumsum(pve), type="o", ylab="Cumulative PVE", xlab="
Principal Component ", col =" brown3 ")
# PAM
# scale genes
sd.data=scale(nci.data)
par(mfrow=c(1,3))
data.dist=dist(sd.data)
data.dist=dist(sd.data) ; data.dist
plot(hclust(data.dist), labels=nci.labs, main="Complete
Linkage", xlab="", sub="",ylab="")
plot(hclust(data.dist, method="average"), labels=nci.labs,
main="Average Linkage", xlab="", sub="",ylab="")
plot(hclust(data.dist, method="single"), labels=nci.labs,
main="Single Linkage", xlab="", sub="",ylab="")
par(mfrow=c(3,1))
data.dist=dist(sd.data) ; data.dist
plot(hclust(data.dist), labels=nci.labs, main="Complete
Linkage", xlab="", sub="",ylab="")
plot(hclust(data.dist, method="average"), labels=nci.labs,
main="Average Linkage", xlab="", sub="",ylab="")
plot(hclust(data.dist, method="single"), labels=nci.labs,
main="Single Linkage", xlab="", sub="",ylab="")
hc.out=hclust(dist(sd.data))
hc.clusters=cutree(hc.out,4)
table(hc.clusters,nci.labs)
par(mfrow=c(1,1))
plot(hc.out, labels=nci.labs)
abline(h=139, col="red")
cutree(hc.out,4)
hc.out
# The same but using kmeans
km.out=kmeans(sd.data, 4, nstart=20)
km.clusters=km.out$cluster
table(km.clusters ,hc.clusters )
hc.out=hclust(dist(pr.out$x[,1:5]))
plot(hc.out, labels=nci.labs, main="Hier. Clust. on First
Five Score Vectors ")
table(cutree(hc.out,4), nci.labs)
find.package("cluster")
library(cluster)
find.package("ISLR")
install.packages("ISLR")
update.packages()
help(USArrests)
summary(USArrests)
head(USArrests)
colnames(USArrests)
row.names(USArrests)<-state.abb
pairs(USArrests)
# PAM clustering with 4 clusters
pam.out<-pam(USArrests,k=4)
clusplot(pam.out,labels=3)
# silhouette width over 0.4 is good
?silhouette
sp<-silhouette(pam.out); sp
plot(sp,col=1:4)
mean(sp[,"sil_width"])
abline(v=mean(sp[,"sil_width"]))
# The mean silhouette width can be used to assess which is the best number of clusters
avesw.vec<-rep(NA,7)
for(i in 2:7)
avesw.vec[i]<-mean(silhouette(pam(USArrests,k=i))[,"sil_width"])
plot(1:7,avesw.vec,type="b",ylim=c(0,0.6))
avesw.vec # cual tiene el largest mean silhouette width? 2???
# Rerun the PAM algorithm with the optimal number of clusters
pam.out<-pam(scale(USArrests),k=2); pam.out
clusplot(pam.out,labels=3)
sp<-silhouette(pam.out)
plot(sp,col=1:2)
abline(v=mean(sp[,"sil_width"]))
# The mean silhouette width can be used to assess which is the best number of clusters
avesw.vec<-rep(NA,7)
for(i in 2:7)
avesw.vec[i]<-mean(silhouette(pam(USArrests,k=i))[,"sil_width"])
plot(1:7,avesw.vec,type="b",ylim=c(0,0.6))
avesw.vec # cual tiene el largest mean silhouette width? 2???
# Rerun the PAM algorithm with the optimal number of clusters
pam.out<-pam(scale(USArrests),k=2); pam.out
# Rerun the PAM algorithm with the optimal number of clusters
pam.out<-pam(scale(USArrests),k=5); pam.out
clusplot(pam.out,labels=3)
sp<-silhouette(pam.out)
plot(sp,col=1:2)
abline(v=mean(sp[,"sil_width"]))
# Compare the optimal PAM clustering with the K-Means result
km.out<-kmeans(scale(USArrests),centers=2,nstart=20); km.out # se debe hacer antes prcomp? no
table(km.out$cluster,pam.out$clustering)
km.out$cluster == pam.out$clustering
# silhouette using kmeans as input
sp<-silhouette(km.out$cluster, dist(scale(USArrests))) # dont forget to scale
plot(sp,col=1:2)
decathlon <- read.csv2("Zehnkampf2017Hamburg.csv",
stringsAsFactors = FALSE, fileEncoding = "ISO-8859-1", header = TRUE, sep = ";")[-c(1:3,seq(10,28,2))]
# silhouette width over 0.4 is good
?silhouette
sp<-silhouette(pam.out); sp
plot(sp,col=1:4)
# PAM clustering with 4 clusters
pam.out<-pam(USArrests,k=4)
clusplot(pam.out,labels=3)
# silhouette width over 0.4 is good
?silhouette
sp<-silhouette(pam.out); sp
plot(sp,col=1:4)
mean(sp[,"sil_width"])
abline(v=mean(sp[,"sil_width"]))
# PAM clustering with 4 clusters
pam.out<-pam(USArrests,k=5)
clusplot(pam.out,labels=3)
# silhouette width over 0.4 is good
?silhouette
sp<-silhouette(pam.out); sp
plot(sp,col=1:4)
# PAM clustering with 4 clusters
pam.out<-pam(USArrests,k=5)
clusplot(pam.out,labels=5)
unique(fights.name$
}
source('~/Documents/Pokemon-winner-prediction/data_preprocessing.r')
#Libraries
library(ggplot2)
library(dplyr)
library(gridExtra)
#install.packages('corrplot')
library(corrplot)
#install.packages('caret')
library(caret)
#install.packages('ggthemes')
library(ggthemes)
library(RColorBrewer)
#install.packages('fmsb')
library(fmsb)
#install.packages('rpart.plot')
library(rpart.plot)
#install.packages('ROCR')
library(ROCR)
#install.packages('tidyverse')
#library(tidyverse)
library(ggplot2)
# Load data
setwd("~/Documents/Pokemon-winner-prediction")
pokemon <- read.csv('./pokemon.csv')
fights <- read.csv('./combats.csv')
#EDV PLots
par(las=2)
barplot(table(pokemon$Type.1),
col = heat.colors(length(unique(pokemon$Type.1))),
main = 'Type 1 Pokemon')
barplot(table(pokemon$Type.2),
col = heat.colors(length(unique(pokemon$Type.2))),
main = 'Type 2 Pokemon')
barplot(table(pokemon$Generation),
col = heat.colors(length(unique(pokemon$Generation))),
main = 'Generation Pokemon')
boxplot(Defense ~ Type.1, data = pokemon,
col = rainbow(length(unique(pokemon$Type.1))))
boxplot(Defense ~ Type.1,
data=pokemon,
main="Defense Range by Pokemon Type",
xlab="Type 1",
ylab="Defense",
col="orange",
border="brown"
)
#nullvalues
colSums(is.na(pokemon))
colSums(is.na(fights))
dim(pokemon)
head(pokemon)
colnames(pokemon)
dim(fights)
head(fights)
colnames(fights)
names <- pokemon[,c(1,2)]; head(names) # join id with pokemon name
colnames(names)
head(names)
#Map the figths table from id to pokemon name
fights.name <- data.frame(lapply(fights, function(x) names$Name[match(x,names$X.)]))
head(fights)
View(fights.name)
wins <- group_by(fights.name, Winner)
summarise(wins, wins1 = n())
